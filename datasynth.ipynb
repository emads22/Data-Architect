{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It89APiAtTUF"
      },
      "source": [
        "# DataSynth\n",
        "\n",
        "Develop a Dataset Generator using Gradio UI to create an intuitive interface that streamlines dataset generation tasks, allowing users to customize and generate datasets effortlessly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxoCPDKay3b"
      },
      "source": [
        "## Setup and Install Dependencies\n",
        "\n",
        "- Start by installing all necessary libraries for dataset generation, model loading, and building the Gradio interface.\n",
        "- Ensure to include dependencies for data preprocessing, visualization, and export capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2vvgnFpHpID",
        "outputId": "9081d13a-3ff2-4e88-9cb8-bf29db9c59d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2 gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import threading\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata, drive\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer, TextStreamer\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTl3mcjyzIEE"
      },
      "source": [
        "## Mount Google Drive to the Colab Environment\n",
        "\n",
        "- Mount Google Drive to enable access to files stored in your Drive directly from the Colab environment.  \n",
        "- This allows saving and loading models, files, and other resources persistently across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es9GkQ0FGCMt",
        "outputId": "191cda75-f6c2-4a90-b391-dff512babf55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive to the Colab environment, allowing access to files stored in the Drive.\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COWdmqYnqVlN"
      },
      "source": [
        "## Define Required Constants\n",
        "\n",
        "- Set up the necessary constants that will be used throughout the application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "outputs": [],
      "source": [
        "# Specifies the path or identifier for the LLaMA model to be used for generating meeting minutes\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# These paths are optional and can be customized based on user preference.\n",
        "\n",
        "# DRIVE_DIR = \"/content\"  # Uncomment this line and comment the next one if you prefer to save the model in the temporary runtime session (non-persistent storage).\n",
        "DRIVE_DIR = \"/content/drive/MyDrive\"  # Path to Google Drive for persistent storage.\n",
        "DRIVE_MODELS_DIR = DRIVE_DIR + \"/my_models\"  # Directory within Google Drive to store the saved models.\n",
        "\n",
        "# End-of-sequence (EOS) token used by the LLaMA model to signify the end of the entire generated response.\n",
        "MODEL_EOS = \"<|eot_id|>\"\n",
        "\n",
        "# Maximum number of tokens allowed for the model's generation to control output length and prevent exceeding limits\n",
        "MAX_TOKENS = 2000\n",
        "\n",
        "# Define the assistant's role and task instructions\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI assistant tasked with generating synthetic testing data for various purposes. Your goal is to create realistic and diverse datasets based on the user's specifications. Follow these instructions carefully to produce high-quality synthetic data in JSON format.\n",
        "\n",
        "Inputs You Will Receive:\n",
        "1. Subject: This specifies the type of data to generate (e.g., \"job postings\", \"customer reviews\", \"movie listings\", ...).\n",
        "2. Number of Samples: The number of synthetic data samples to generate.\n",
        "\n",
        "Your Steps:\n",
        "1. Analyze the provided subject and number of samples.\n",
        "2. Identify and define a consistent set of fields relevant to the subject. These fields should be directly related to the subject and serve as a template for the dataset's format, relationships, and conventions. Ensure the identified fields are meaningful, logical, and contextually appropriate for the subject.\n",
        "3. Generate synthetic data for each field within every sample, ensuring the structure is consistent across all samples.\n",
        "4. For each field, generate realistic and diverse content that matches the specified data type and examples. Ensure consistency within each sample while introducing appropriate variations across the dataset.\n",
        "5. Add slight randomness and imperfections, such as varying lengths, occasional typos, or minor inconsistencies, to make the data appear more realistic.\n",
        "6. Do not include any placeholder text like \"Lorem ipsum\" or other generic patterns. All generated content must be realistic and contextually appropriate.\n",
        "\n",
        "Output Format:\n",
        "- The generated data must be output as a JSON object.\n",
        "- Use the following structure to format the output JSON dataset:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"field1\": \"value1\",\n",
        "      \"field2\": \"value2\",\n",
        "      \"field3\": \"value3\",\n",
        "      ...\n",
        "    },\n",
        "    {\n",
        "      \"field1\": \"value1\",\n",
        "      \"field2\": \"value2\",\n",
        "      \"field3\": \"value3\",\n",
        "      ...\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Important Directive:\n",
        "- Do not include any explanatory text, labels, or formatting (such as Markdown) in the output.\n",
        "- The output must strictly consist of the JSON dataset without any additional context, headings, or commentary.\n",
        "\n",
        "If any required input (subject or number of samples) is missing or incomplete, do not proceed with generating the data. Instead, prompt the user to provide all necessary details before continuing.\n",
        "\n",
        "Your primary objectives are:\n",
        "- Ensure the output is diverse and realistic.\n",
        "- Maintain adherence to the conventions and structure defined in the Output Format.\n",
        "- Identify and include meaningful fields related to the subject, keeping their structure consistent across all samples.\n",
        "- Deliver the specified number of samples in the expected JSON format.\n",
        "- Output only the JSON dataset with no surrounding text, explanations, or formatting.\n",
        "\n",
        "Always strive to produce data that appears natural, adheres to the expectations set by the examples, and avoids the use of placeholder text or generic content.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Construct the user's prompt with detailed instructions and the provided transcript\n",
        "def user_prompt(subject: str, num_samples: int) -> str:\n",
        "    prompt = f\"\"\"\n",
        "Input Specifications:\n",
        "- Subject: {subject}\n",
        "- Number of Samples: {num_samples}\n",
        "\n",
        "Task Overview:\n",
        "You will generate synthetic testing data based on the provided subject and number of samples. Each data sample must follow a structured format with consistent fields relevant to the subject.\n",
        "\n",
        "Data Structure Template:\n",
        "{{\n",
        "  \"field1\": \"value1\",\n",
        "  \"field2\": \"value2\",\n",
        "  \"field3\": \"value3\",\n",
        "  ...\n",
        "}}\n",
        "\n",
        "Instructions:\n",
        "1. Define meaningful and relevant fields based on the subject. Ensure these fields remain consistent across all samples.\n",
        "2. Generate realistic and diverse data for each field, introducing appropriate variations while maintaining logical relationships.\n",
        "3. Add slight randomness (e.g., variations in length, occasional typos, or varying styles) to enhance realism.\n",
        "4. Ensure the output strictly adheres to the structure and includes the specified number of samples.\n",
        "\n",
        "Sample Output:\n",
        "{{\n",
        "  \"data\": [\n",
        "    {{\n",
        "      \"Field Name 1\": \"Example Value A\",\n",
        "      \"Field Name 2\": \"Example Value B\",\n",
        "      \"Field Name 3\": \"Example Value C\"\n",
        "    }},\n",
        "    {{\n",
        "      \"Field Name 1\": \"Example Value X\",\n",
        "      \"Field Name 2\": \"Example Value Y\",\n",
        "      \"Field Name 3\": \"Example Value Z\"\n",
        "    }},\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8TtmIfkjfju"
      },
      "source": [
        "## Add Secrets to the Colab Notebook\n",
        "\n",
        "- Add your Hugging Face Hub credentials to sign in and access models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "outputs": [],
      "source": [
        "# Sign in to HuggingFace Hub using Secrets in Colab\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbnNjDh8FDOr"
      },
      "source": [
        "## Define Model and Tokenizer Variables\n",
        "\n",
        "- Initialize the `model` and `tokenizer` variables by assigning them to `None`.  \n",
        "- This ensures they can be properly loaded later when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d0VrTdBR9i57"
      },
      "outputs": [],
      "source": [
        "# Initialize the model and tokenizer variables\n",
        "tokenizer = None\n",
        "model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA1-Rs5CtGeO"
      },
      "source": [
        "## Load the Model and Tokenizer\n",
        "\n",
        "- If this is the first time using the runtime, load the model from the Hugging Face Hub and save it to the drive for future use (this ensures the model persists even after the runtime disconnects).  \n",
        "- Alternatively, the model can be saved in the current temporary runtime session location, but note that it will not persist after the session ends or disconnects.  \n",
        "- If the model is already saved on the drive, it will be loaded directly from there to save time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hMwQuGd76u3M"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name, local_dir=DRIVE_MODELS_DIR):\n",
        "    # Convert the local_dir to a Path object for easier path handling\n",
        "    local_dir = Path(local_dir)\n",
        "\n",
        "    # Create a subdirectory for the model, replacing '/' in model_name with '_'\n",
        "    model_dir = local_dir / model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    if model_dir.exists():  # Check if the model is already downloaded locally\n",
        "        # Load the tokenizer and model from the existing directory\n",
        "        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
        "        model = AutoModelForCausalLM.from_pretrained(str(model_dir))\n",
        "\n",
        "    else:  # If the model is not available locally, download and configure it\n",
        "        # Configure the quantization settings for loading the model in 4-bit precision\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                   # Load the model in 4-bit precision to save memory\n",
        "            bnb_4bit_use_double_quant=True,      # Enable double quantization for better performance\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # Set computation data type to bfloat16\n",
        "            bnb_4bit_quant_type=\"nf4\"            # Use NF4 quantization type for improved accuracy\n",
        "        )\n",
        "\n",
        "        # Download the tokenizer with remote code support enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "        # Set the padding token to the end-of-sequence (EOS) token for consistency\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Download and load the model with the specified quantization configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, config=quant_config)\n",
        "\n",
        "        # Save the downloaded model and tokenizer locally for future use\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "        model.save_pretrained(model_dir)  # Save the model to the specified directory\n",
        "        tokenizer.save_pretrained(model_dir)  # Save the tokenizer to the specified directory\n",
        "\n",
        "    # Return the loaded tokenizer and model for further use\n",
        "    return tokenizer, model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ff7a5b21a7d0419187b5a3d389e683ee",
            "df8ba506f3324b58a32d0d9760d0d6a1",
            "8dd7ea4438024d3294e08cac70b0a8d1",
            "3ee8ea26c6654c85bb0dd089aa39445c",
            "6c1f6ec89cf94d5c908ca70aa35f9de7",
            "46988e2333ad47fe9b881f0e96a6bdda",
            "7059c7283a1d45f392b2c6d61b5f9da5",
            "a7679840a7f14df5ab3f2b21a556313e",
            "b70330d0f0bd49948518bf59002e8eb1",
            "100bc5185c6b4e91875bbff6db8a8a59",
            "09a9956cf4a34521ad74f6c017f5cf2b"
          ]
        },
        "id": "hsPIT2ydgbH7",
        "outputId": "2a53d987-908e-4f38-f8c9-b6c6cfdbd3bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff7a5b21a7d0419187b5a3d389e683ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# call the helper function and load the model and tokenizer\n",
        "tokenizer, model = load_model(LLAMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKxNjzFDUffs"
      },
      "source": [
        "## Test Dataset Generation\n",
        "\n",
        "- Easily generate synthetic datasets based on your test inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6yaXO1Ycuf0x"
      },
      "outputs": [],
      "source": [
        "# generating test function\n",
        "def test_generate(user_prompt):\n",
        "  global tokenizer, model\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT }]\n",
        "  # Append the user's new message to the conversation history\n",
        "  messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, decode_kwargs={\"skip_special_tokens\": True})\n",
        "  # model.generate(inputs, max_new_tokens=MAX_TOKENS, streamer=streamer)\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,\n",
        "      kwargs={\"inputs\": inputs, \"max_new_tokens\": MAX_TOKENS, \"streamer\": streamer}\n",
        "  )\n",
        "  thread.start()\n",
        "\n",
        "  full_response = \"\"\n",
        "  for chunk in streamer:\n",
        "    cleaned_chunk = chunk.replace(MODEL_EOS, \"\")\n",
        "    print(cleaned_chunk, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b06Q4Hhaot0S",
        "outputId": "3c5004f3-6895-4e35-8e47-546d921161a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"Rating\": 4,\n",
            "      \"Review Title\": \"Excellent Service!\",\n",
            "      \"Review Text\": \"I recently visited this store and was blown away by the friendly staff and excellent customer service. They went above and beyond to help me find what I needed.\",\n",
            "      \"Product Name\": \"Smartwatch\",\n",
            "      \"Product Rating\": 5,\n",
            "      \"Purchase Date\": \"2023-02-20\",\n",
            "      \"Overall Satisfaction\": \"Very Satisfied\"\n",
            "    },\n",
            "    {\n",
            "      \"Rating\": 2,\n",
            "      \"Review Title\": \"Disappointing Experience\",\n",
            "      \"Review Text\": \"I ordered this product online, but when it arrived, it was damaged. The customer service was unhelpful and took a long time to respond. Needless to say, I won't be purchasing from this company again.\",\n",
            "      \"Product Name\": \"Smartphone\",\n",
            "      \"Product Rating\": 1,\n",
            "      \"Purchase Date\": \"2023-03-15\",\n",
            "      \"Overall Satisfaction\": \"Neutral\"\n",
            "    },\n",
            "    {\n",
            "      \"Rating\": 5,\n",
            "      \"Review Title\": \"Love My Purchase!\",\n",
            "      \"Review Text\": \"I've been using this product for a few months now, and I'm thoroughly impressed. It's been a game-changer for my daily routine. The quality is top-notch, and the price is very reasonable.\",\n",
            "      \"Product Name\": \"Wireless Earbuds\",\n",
            "      \"Product Rating\": 5,\n",
            "      \"Purchase Date\": \"2023-01-01\",\n",
            "      \"Overall Satisfaction\": \"Extremely Satisfied\"\n",
            "    },\n",
            "    {\n",
            "      \"Rating\": 3,\n",
            "      \"Review Title\": \"Average Product\",\n",
            "      \"Review Text\": \"This product does what it says it will, but it's not particularly impressive. The build quality is decent, but the features are somewhat limited. I'd recommend it to others, but it's not a must-have.\",\n",
            "      \"Product Name\": \"Tablet\",\n",
            "      \"Product Rating\": 3,\n",
            "      \"Purchase Date\": \"2023-04-10\",\n",
            "      \"Overall Satisfaction\": \"Satisfied\"\n",
            "    },\n",
            "    {\n",
            "      \"Rating\": 1,\n",
            "      \"Review Title\": \"Terrible Product\",\n",
            "      \"Review Text\": \"I'm extremely disappointed with this product. It's slow, buggy, and lacks essential features. The customer support is non-existent. I'd advise others to avoid this product at all costs.\",\n",
            "      \"Product Name\": \"Laptop\",\n",
            "      \"Product Rating\": 1,\n",
            "      \"Purchase Date\": \"2023-05-25\",\n",
            "      \"Overall Satisfaction\": \"Very Dissatisfied\"\n",
            "    },\n",
            "    {\n",
            "      \"Rating\": 4,\n",
            "      \"Review Title\": \"Great Value\",\n",
            "      \"Review Text\": \"This product offers great value for the price. It's a solid choice for those on a budget. The features are decent, and the build quality is acceptable. I'd recommend it to others.\",\n",
            "      \"Product Name\": \"Smart Speaker\",\n",
            "      \"Product Rating\": 4,\n",
            "      \"Purchase Date\": \"2023-06-01\",\n",
            "      \"Overall Satisfaction\": \"Satisfied\"\n",
            "    }\n",
            "  ]\n",
            "}"
          ]
        }
      ],
      "source": [
        "# call the function with testing inputs values\n",
        "test_generate(user_prompt(subject = \"customer reviews\", num_samples = 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjdHkJgpfJt"
      },
      "source": [
        "## Create the User Interface (UI) with Gradio\n",
        "\n",
        "- Design an intuitive and user-friendly Gradio interface to generate synthetic datasets effortlessly.\n",
        "- Allow users to specify inputs, customize fields, and generate datasets in real-time.\n",
        "- Ensure the UI is simple, responsive, and accessible for seamless interaction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nheHOEtsw1Ki"
      },
      "outputs": [],
      "source": [
        "# Function to process input data\n",
        "def process_user_inputs(subject, num_samples):\n",
        "    # Ensure all required inputs are provided\n",
        "    if not subject:\n",
        "        yield \"Please provide the Subject.\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    if not num_samples:\n",
        "        yield \"Please provide the Number of Samples.\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    # Generate data based on validated inputs\n",
        "    try:\n",
        "        yield from generate_dataset(subject, num_samples)\n",
        "    except Exception as e:\n",
        "        yield f\"Error during data generation: {e}\"\n",
        "        return  # Terminate the function after yielding the error message to prevent further execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nOCYpSE2YTlD"
      },
      "outputs": [],
      "source": [
        "# Function to generate synthetic data\n",
        "def generate_dataset(subject, num_samples):\n",
        "    global tokenizer, model  # Use the globally defined tokenizer and model\n",
        "\n",
        "    # Construct messages for the chat template: the system prompt and user-specific instructions\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": SYSTEM_PROMPT},  # System-level instructions\n",
        "        {\"role\": \"user\", \"content\": user_prompt(subject, num_samples)}  # User-defined parameters\n",
        "    ]\n",
        "\n",
        "    # Prepare the input data for the model using the tokenizer\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",  # Return as PyTorch tensors\n",
        "        add_generation_prompt=True  # Include a generation prompt for the model\n",
        "    ).to(\"cuda\")  # Send data to the GPU for faster processing\n",
        "\n",
        "    # Set up a streamer to handle the token decoding process\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,  # The tokenizer used for decoding\n",
        "        skip_prompt=True,  # Skip the prompt text in the output\n",
        "        decode_kwargs={\"skip_special_tokens\": True}  # Exclude special tokens during decoding\n",
        "    )\n",
        "\n",
        "    # Create a thread to handle text generation asynchronously\n",
        "    thread = threading.Thread(\n",
        "        target=model.generate,  # Use the model's generate method\n",
        "        kwargs={\n",
        "            \"inputs\": inputs,  # Provide the prepared input tensors\n",
        "            \"max_new_tokens\": MAX_TOKENS,  # Limit the maximum number of tokens to generate\n",
        "            \"streamer\": streamer  # Stream the generated tokens using the streamer\n",
        "        }\n",
        "    )\n",
        "\n",
        "    thread.start()  # Start the generation thread\n",
        "\n",
        "    full_response = \"\"  # Initialize an empty string to store the full response\n",
        "    for chunk in streamer:  # Iterate over the streamed chunks of generated text\n",
        "        cleaned_chunk = chunk.replace(MODEL_EOS, \"\")  # Remove any end-of-sequence markers\n",
        "        full_response += cleaned_chunk  # Append the cleaned chunk to the full response\n",
        "        yield full_response  # Yield the accumulated response in real-time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pgTXRK7hwUKT"
      },
      "outputs": [],
      "source": [
        "# Gradio UI function\n",
        "def gradio_ui():\n",
        "    with gr.Blocks() as ui:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # Data Architect\n",
        "\n",
        "        Welcome to the `Data Architect`! Use this tool to create realistic and diverse datasets for testing purposes. Simply provide the required information below, including sample field names and values, and let the system generate synthetic data for you.\n",
        "        <br><br>\n",
        "        \"\"\")\n",
        "\n",
        "        # User inputs and output in a single column\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Define General Information\n",
        "        Provide the subject (e.g., Job Postings, Customer Reviews, Movie Listings) and the number of samples you wish to generate. This will guide the synthetic data generation process.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            subject = gr.Textbox(\n",
        "                label=\"Subject\",\n",
        "                placeholder=\"Enter the subject of the data\",\n",
        "                value=\"\"\n",
        "            )\n",
        "            num_samples = gr.Slider(\n",
        "              minimum=1,\n",
        "              maximum=100,\n",
        "              step=1,\n",
        "              value=6,\n",
        "              label=\"Number of Samples\"\n",
        "          )\n",
        "\n",
        "        # Button to trigger generation\n",
        "        generate_button = gr.Button(\"Generate Synthetic Data\")\n",
        "\n",
        "        # Markdown for final output\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Generated Dataset\n",
        "        ### The final aggregated dataset will appear below in JSON format. You can select and copy this data for your testing, development, or deployment needs.\n",
        "        \"\"\")\n",
        "        output = gr.TextArea(\n",
        "            label=\"Generated Dataset\",\n",
        "            interactive=False,  # Read-only\n",
        "            lines=15\n",
        "        )\n",
        "\n",
        "        # Bind button click to processing function\n",
        "        generate_button.click(\n",
        "            fn=process_user_inputs,\n",
        "            inputs=[subject, num_samples],\n",
        "            outputs=output\n",
        "        )\n",
        "\n",
        "    return ui\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "eVs6Bhp_wUGC",
        "outputId": "63da3217-5ca9-4769-db61-0965f4f1ae39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://88855931211e8d5582.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://88855931211e8d5582.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Launch the UI\n",
        "ui = gradio_ui()\n",
        "ui.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7VQkGygFBYJ"
      },
      "source": [
        "## Contributing\n",
        "Contributions are welcome! Here are some ways you can contribute to the project:\n",
        "- Report bugs and issues.\n",
        "- Suggest new features or improvements.\n",
        "- Submit pull requests with bug fixes or enhancements.\n",
        "\n",
        "You can contribute to this project by visiting the [GitHub repository](https://github.com/emads22/MeetingRecap).\n",
        "\n",
        "## Author\n",
        "- **Emad**  \n",
        "  [<img src=\"https://img.shields.io/badge/GitHub-Profile-blue?logo=github\" width=\"150\">](https://github.com/emads22)\n",
        "\n",
        "## License\n",
        "This project is licensed under the MIT License, which grants permission for free use, modification, distribution, and sublicense of the code, provided that the copyright notice (attributed to [emads22](https://github.com/emads22)) and permission notice are included in all copies or substantial portions of the software. This license is permissive and allows users to utilize the code for both commercial and non-commercial purposes.\n",
        "\n",
        "Please see the [LICENSE](https://github.com/emads22/MeetingRecap/blob/main/LICENSE) file for more details.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09a9956cf4a34521ad74f6c017f5cf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "100bc5185c6b4e91875bbff6db8a8a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee8ea26c6654c85bb0dd089aa39445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_100bc5185c6b4e91875bbff6db8a8a59",
            "placeholder": "​",
            "style": "IPY_MODEL_09a9956cf4a34521ad74f6c017f5cf2b",
            "value": " 2/2 [01:33&lt;00:00, 41.50s/it]"
          }
        },
        "46988e2333ad47fe9b881f0e96a6bdda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1f6ec89cf94d5c908ca70aa35f9de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7059c7283a1d45f392b2c6d61b5f9da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dd7ea4438024d3294e08cac70b0a8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7679840a7f14df5ab3f2b21a556313e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b70330d0f0bd49948518bf59002e8eb1",
            "value": 2
          }
        },
        "a7679840a7f14df5ab3f2b21a556313e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b70330d0f0bd49948518bf59002e8eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df8ba506f3324b58a32d0d9760d0d6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46988e2333ad47fe9b881f0e96a6bdda",
            "placeholder": "​",
            "style": "IPY_MODEL_7059c7283a1d45f392b2c6d61b5f9da5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ff7a5b21a7d0419187b5a3d389e683ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df8ba506f3324b58a32d0d9760d0d6a1",
              "IPY_MODEL_8dd7ea4438024d3294e08cac70b0a8d1",
              "IPY_MODEL_3ee8ea26c6654c85bb0dd089aa39445c"
            ],
            "layout": "IPY_MODEL_6c1f6ec89cf94d5c908ca70aa35f9de7"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
