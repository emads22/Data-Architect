{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f80480d706e4d75ad5ae06d49251f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a4d63cd9c4142e88c2635dece8f58c7",
              "IPY_MODEL_f62fc6527aca421bb4f96217b4dcf78a",
              "IPY_MODEL_1745f5766f80430ca438c185e526a3f9"
            ],
            "layout": "IPY_MODEL_8b24fb73df9b46fa821fb45656be5a82"
          }
        },
        "5a4d63cd9c4142e88c2635dece8f58c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad4e5321160e48c3a6ad6fa7843a4d17",
            "placeholder": "​",
            "style": "IPY_MODEL_7d27e9b1a8424607807efecf05050cca",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f62fc6527aca421bb4f96217b4dcf78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0478a6ca0fbc4f8eb934c517c8613b29",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2f473eccdd84b8883820ef069fa76bf",
            "value": 2
          }
        },
        "1745f5766f80430ca438c185e526a3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896f6e714bd54f94889ae85f1b8282d7",
            "placeholder": "​",
            "style": "IPY_MODEL_8d982b0ed7a04caeb790ff0e38f52534",
            "value": " 2/2 [02:15&lt;00:00, 59.72s/it]"
          }
        },
        "8b24fb73df9b46fa821fb45656be5a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad4e5321160e48c3a6ad6fa7843a4d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d27e9b1a8424607807efecf05050cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0478a6ca0fbc4f8eb934c517c8613b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f473eccdd84b8883820ef069fa76bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "896f6e714bd54f94889ae85f1b8282d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d982b0ed7a04caeb790ff0e38f52534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Architect\n",
        "\n",
        "Develop a Dataset Generator using Gradio UI to create an intuitive interface that streamlines dataset generation tasks, allowing users to customize and generate datasets effortlessly.\n"
      ],
      "metadata": {
        "id": "It89APiAtTUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Install Dependencies\n",
        "\n",
        "- Start by installing all necessary libraries for dataset generation, model loading, and building the Gradio interface.\n",
        "- Ensure to include dependencies for data preprocessing, visualization, and export capabilities."
      ],
      "metadata": {
        "id": "RCxoCPDKay3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2 gradio"
      ],
      "metadata": {
        "id": "f2vvgnFpHpID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae059082-b814-416b-c6ea-c4dc137a590b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.4/321.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import threading\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata, drive\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer, TextStreamer\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive to the Colab Environment\n",
        "\n",
        "- Mount Google Drive to enable access to files stored in your Drive directly from the Colab environment.  \n",
        "- This allows saving and loading models, files, and other resources persistently across sessions.\n"
      ],
      "metadata": {
        "id": "HTl3mcjyzIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive to the Colab environment, allowing access to files stored in the Drive.\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Es9GkQ0FGCMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ea811c-761f-48d3-c3d9-1a255472ccd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Required Constants\n",
        "\n",
        "- Set up the necessary constants that will be used throughout the application.\n"
      ],
      "metadata": {
        "id": "COWdmqYnqVlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifies the path or identifier for the LLaMA model to be used for generating meeting minutes\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# These paths are optional and can be customized based on user preference.\n",
        "\n",
        "# DRIVE_DIR = \"/content\"  # Uncomment this line and comment the next one if you prefer to save the model in the temporary runtime session (non-persistent storage).\n",
        "DRIVE_DIR = \"/content/drive/MyDrive\"  # Path to Google Drive for persistent storage.\n",
        "DRIVE_MODELS_DIR = DRIVE_DIR + \"/my_models\"  # Directory within Google Drive to store the saved models.\n",
        "\n",
        "# End-of-sequence (EOS) token used by the LLaMA model to signify the end of the entire generated response.\n",
        "MODEL_EOS = \"<|eot_id|>\"\n",
        "\n",
        "# Maximum number of tokens allowed for the model's generation to control output length and prevent exceeding limits\n",
        "MAX_TOKENS = 2000\n",
        "\n",
        "# Define the assistant's role and task instructions\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an AI assistant tasked with generating synthetic testing data for various purposes. Your goal is to create realistic and diverse datasets based on the user's specifications and provided examples. Follow these instructions carefully to produce high-quality synthetic data in JSON format.\n",
        "\n",
        "Inputs You Will Receive:\n",
        "1. Subject: This specifies the type of data to generate (e.g., \"job postings\", \"customer reviews\").\n",
        "2. Number of Samples: The number of synthetic data samples to generate.\n",
        "3. Two Multi-Shot Examples: These examples define the expected structure and style of the data to be generated. Each example will include three fields (field1, field2, field3) and their corresponding values. If fewer than three fields are required, unused fields will be left blank or set to `None`.\n",
        "\n",
        "Your Steps:\n",
        "1. Analyze the provided subject, number of samples, and the structure of the examples.\n",
        "2. Use the examples as a guide to replicate the format, relationships, and conventions for the synthetic data.\n",
        "3. For each field, generate realistic and diverse content that matches the specified data type and examples. Ensure consistency within each sample while introducing appropriate variations across the dataset.\n",
        "4. Add slight randomness and imperfections, such as varying lengths, occasional typos, or minor inconsistencies, to make the data appear more realistic.\n",
        "5. If a field is left blank or set to `None` in the examples, ensure this is reflected in the generated data where applicable.\n",
        "6. Do not include any placeholder text like \"Lorem ipsum\" or other generic patterns. All generated content must be realistic and contextually appropriate.\n",
        "\n",
        "Output Format:\n",
        "- The generated data must be output as a JSON object.\n",
        "- Use the following structure to format the output JSON dataset:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"field1\": \"value1\",\n",
        "      \"field2\": \"value2\",\n",
        "      \"field3\": \"value3\"\n",
        "    },\n",
        "    {\n",
        "      \"field1\": \"value1\",\n",
        "      \"field2\": \"value2\",\n",
        "      \"field3\": \"value3\"\n",
        "    },\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Important Directive:\n",
        "- Do not include any explanatory text, labels, or formatting (such as Markdown) in the output.\n",
        "- The output must strictly consist of the JSON dataset without any additional context, headings, or commentary.\n",
        "\n",
        "If any required input (subject, number of samples, or examples) is missing or incomplete, do not proceed with generating the data. Instead, prompt the user to provide all necessary details before continuing.\n",
        "\n",
        "Your primary objectives are:\n",
        "- Ensure the output is diverse and realistic.\n",
        "- Maintain adherence to the conventions and structure defined in the examples.\n",
        "- Deliver the specified number of samples in the expected JSON format.\n",
        "- Output only the JSON dataset with no surrounding text, explanations, or formatting.\n",
        "\n",
        "Always strive to produce data that appears natural, adheres to the expectations set by the examples, and avoids the use of placeholder text or generic content.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Construct the user's prompt with detailed instructions and the provided transcript\n",
        "def user_prompt(subject: str, num_samples: int, sample1: dict, sample2: dict, sample3: dict) -> str:\n",
        "    prompt = f\"\"\"\n",
        "Input Specifications:\n",
        "- Subject: {subject}\n",
        "- Number of Samples: {num_samples}\n",
        "\n",
        "This will guide you to structure the generated data correctly.\n",
        "\n",
        "Each synthetic data sample will follow this structure:\n",
        "\n",
        "{{\n",
        "  \"field1\": \"value1\",\n",
        "  \"field2\": \"value2\",\n",
        "  \"field3\": \"value3\"\n",
        "}}\n",
        "\n",
        "These are Three Multi-Shot Examples:\n",
        "\n",
        "Example of Sample 1:\n",
        "{sample1}\n",
        "\n",
        "Example of Sample 2:\n",
        "{sample2}\n",
        "\n",
        "Example of Sample 3:\n",
        "{sample3}\n",
        "\n",
        "Do not include these provided examples in the generated output.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Using the provided subject, number of samples, fields, and examples:\n",
        "\n",
        "1. Generate synthetic testing data that adheres to the structure defined above.\n",
        "2. Maintain diversity and realism in the generated data, ensuring that it matches the conventions, formatting, and relationships specified in the examples.\n",
        "3. Introduce slight randomness (e.g., variations in length, occasional typos, or varying content) to make the data more realistic.\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "{{\n",
        "  \"data\": [\n",
        "    {{\n",
        "      \"Sample Field 1\": \"Sample Value A\",\n",
        "      \"Sample Field 2\": \"Sample Value B\",\n",
        "      \"Sample Field 3\": \"Sample Value C\"\n",
        "    }},\n",
        "    {{\n",
        "      \"Sample Field 1\": \"Sample Value X\",\n",
        "      \"Sample Field 2\": \"Sample Value Y\",\n",
        "      \"Sample Field 3\": \"Sample Value Z\"\n",
        "    }},\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Secrets to the Colab Notebook\n",
        "\n",
        "- Add your Hugging Face Hub credentials to sign in and access models.  "
      ],
      "metadata": {
        "id": "Q8TtmIfkjfju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub using Secrets in Colab\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model and Tokenizer Variables\n",
        "\n",
        "- Initialize the `model` and `tokenizer` variables by assigning them to `None`.  \n",
        "- This ensures they can be properly loaded later when needed."
      ],
      "metadata": {
        "id": "fbnNjDh8FDOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and tokenizer variables\n",
        "tokenizer = None\n",
        "model = None"
      ],
      "metadata": {
        "id": "d0VrTdBR9i57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model and Tokenizer\n",
        "\n",
        "- If this is the first time using the runtime, load the model from the Hugging Face Hub and save it to the drive for future use (this ensures the model persists even after the runtime disconnects).  \n",
        "- Alternatively, the model can be saved in the current temporary runtime session location, but note that it will not persist after the session ends or disconnects.  \n",
        "- If the model is already saved on the drive, it will be loaded directly from there to save time.\n"
      ],
      "metadata": {
        "id": "hA1-Rs5CtGeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, local_dir=DRIVE_MODELS_DIR):\n",
        "    # Convert the local_dir to a Path object for easier path handling\n",
        "    local_dir = Path(local_dir)\n",
        "\n",
        "    # Create a subdirectory for the model, replacing '/' in model_name with '_'\n",
        "    model_dir = local_dir / model_name.replace(\"/\", \"_\")\n",
        "\n",
        "    if model_dir.exists():  # Check if the model is already downloaded locally\n",
        "        # Load the tokenizer and model from the existing directory\n",
        "        tokenizer = AutoTokenizer.from_pretrained(str(model_dir))\n",
        "        model = AutoModelForCausalLM.from_pretrained(str(model_dir))\n",
        "\n",
        "    else:  # If the model is not available locally, download and configure it\n",
        "        # Configure the quantization settings for loading the model in 4-bit precision\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                   # Load the model in 4-bit precision to save memory\n",
        "            bnb_4bit_use_double_quant=True,      # Enable double quantization for better performance\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # Set computation data type to bfloat16\n",
        "            bnb_4bit_quant_type=\"nf4\"            # Use NF4 quantization type for improved accuracy\n",
        "        )\n",
        "\n",
        "        # Download the tokenizer with remote code support enabled\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "        # Set the padding token to the end-of-sequence (EOS) token for consistency\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Download and load the model with the specified quantization configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, config=quant_config)\n",
        "\n",
        "        # Save the downloaded model and tokenizer locally for future use\n",
        "        model_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "        model.save_pretrained(model_dir)  # Save the model to the specified directory\n",
        "        tokenizer.save_pretrained(model_dir)  # Save the tokenizer to the specified directory\n",
        "\n",
        "    # Return the loaded tokenizer and model for further use\n",
        "    return tokenizer, model\n",
        "\n"
      ],
      "metadata": {
        "id": "hMwQuGd76u3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the helper function and load the model and tokenizer\n",
        "tokenizer, model = load_model(LLAMA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4f80480d706e4d75ad5ae06d49251f5e",
            "5a4d63cd9c4142e88c2635dece8f58c7",
            "f62fc6527aca421bb4f96217b4dcf78a",
            "1745f5766f80430ca438c185e526a3f9",
            "8b24fb73df9b46fa821fb45656be5a82",
            "ad4e5321160e48c3a6ad6fa7843a4d17",
            "7d27e9b1a8424607807efecf05050cca",
            "0478a6ca0fbc4f8eb934c517c8613b29",
            "e2f473eccdd84b8883820ef069fa76bf",
            "896f6e714bd54f94889ae85f1b8282d7",
            "8d982b0ed7a04caeb790ff0e38f52534"
          ]
        },
        "id": "hsPIT2ydgbH7",
        "outputId": "ac26136f-cc0d-45d9-e270-5caa45df3849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f80480d706e4d75ad5ae06d49251f5e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset Generation\n",
        "\n",
        "- Easily generate synthetic datasets based on your test inputs."
      ],
      "metadata": {
        "id": "qKxNjzFDUffs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating test function\n",
        "def test_generate(user_prompt):\n",
        "  global tokenizer, model\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT }]\n",
        "  # Append the user's new message to the conversation history\n",
        "  messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, decode_kwargs={\"skip_special_tokens\": True})\n",
        "  # model.generate(inputs, max_new_tokens=MAX_TOKENS, streamer=streamer)\n",
        "  thread = threading.Thread(\n",
        "      target=model.generate,\n",
        "      kwargs={\"inputs\": inputs, \"max_new_tokens\": MAX_TOKENS, \"streamer\": streamer}\n",
        "  )\n",
        "  thread.start()\n",
        "\n",
        "  full_response = \"\"\n",
        "  for chunk in streamer:\n",
        "    cleaned_chunk = chunk.replace(MODEL_EOS, \"\")\n",
        "    print(cleaned_chunk, end=\"\")"
      ],
      "metadata": {
        "id": "6yaXO1Ycuf0x"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing inputs values\n",
        "subject = \"customer reviews\"\n",
        "num_samples = 6\n",
        "sample1 = {\n",
        "    \"product\": \"chocolate cake\",\n",
        "    \"review\": \"This cake is the best I've ever tasted! It's moist and flavorful. I bought it for my birthday party and everyone loved it.\",\n",
        "    \"customer\": \"John\",\n",
        "}\n",
        "sample2 = {\n",
        "    \"product\": \"comic book\",\n",
        "    \"review\": \"This book is amazing! The story is so interesting and the pictures are beautiful. I read it to my kids every night.\",\n",
        "    \"customer\": \"Jane\",\n",
        "}\n",
        "sample3 = {\n",
        "    \"product\": \"toy car\",\n",
        "    \"review\": \"I love this toy! It's so colorful and fun to play with. My kids enjoy it every day.\",\n",
        "    \"customer\": \"Jack\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "1OyC-8J-oWtx"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the function\n",
        "test_generate(user_prompt(subject, num_samples, sample1, sample2, sample3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b06Q4Hhaot0S",
        "outputId": "d5ef6a78-0e49-4a24-a86d-b68a673dfcf2"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"data\": [\n",
            "        {\n",
            "            \"product\": \"strawberry shortcake\",\n",
            "            \"review\": \"This cake is absolutely divine! It's sweet and tangy with the perfect balance of flavors. I bought it for my sister's birthday and she loved it.\",\n",
            "            \"customer\": \"Emily\"\n",
            "        },\n",
            "        {\n",
            "            \"product\": \"puzzle book\",\n",
            "            \"review\": \"I'm really enjoying this puzzle book. The pictures are so colorful and the puzzles are challenging but fun. I'm solving it with my kids every evening.\",\n",
            "            \"customer\": \"Mike\"\n",
            "        },\n",
            "        {\n",
            "            \"product\": \"gourmet coffee\",\n",
            "            \"review\": \"This coffee is so rich and flavorful! It's the perfect way to start my day. I'm a bit of a coffee connoisseur and I can appreciate the high-quality beans used in this blend.\",\n",
            "            \"customer\": \"Sarah\"\n",
            "        },\n",
            "        {\n",
            "            \"product\": \"board game\",\n",
            "            \"review\": \"I love playing board games with my friends. This one is a classic and always a hit. The game is so well-designed and the components are of high quality.\",\n",
            "            \"customer\": \"David\"\n",
            "        },\n",
            "        {\n",
            "            \"product\": \"ice cream\",\n",
            "            \"review\": \"This ice cream is amazing! The flavors are so unique and delicious. I'm a big fan of trying new ice cream flavors and this one did not disappoint.\",\n",
            "            \"customer\": \"Kevin\"\n",
            "        },\n",
            "        {\n",
            "            \"product\": \"video game\",\n",
            "            \"review\": \"I've been playing this game nonstop since I got it. The graphics are so realistic and the storyline is engaging. I'm on my third playthrough and I'm still finding new things to discover.\",\n",
            "            \"customer\": \"Lisa\"\n",
            "        }\n",
            "    ]\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the User Interface (UI) with Gradio\n",
        "\n",
        "- Design an intuitive and user-friendly Gradio interface to generate synthetic datasets effortlessly.\n",
        "- Allow users to specify inputs, customize fields, and generate datasets in real-time.\n",
        "- Ensure the UI is simple, responsive, and accessible for seamless interaction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9IjdHkJgpfJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process input data\n",
        "def process_user_inputs(subject, num_samples,\n",
        "                        sample_field1, sample_field2, sample_field3,\n",
        "                        sample1_value1, sample1_value2, sample1_value3,\n",
        "                        sample2_value1, sample2_value2, sample2_value3,\n",
        "                        sample3_value1, sample3_value2, sample3_value3):\n",
        "    # Ensure all required inputs are provided\n",
        "    if not subject:\n",
        "        yield \"Please provide the Subject.\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    if not num_samples:\n",
        "        yield \"Please provide the Number of Samples.\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    if not all([sample_field1, sample_field2, sample_field3]):\n",
        "        yield \"Please provide all Sample Fields (Field 1, Field 2, Field 3).\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    if not all([sample1_value1, sample1_value2, sample1_value3,\n",
        "                sample2_value1, sample2_value2, sample2_value3,\n",
        "                sample3_value1, sample3_value2, sample3_value3]):\n",
        "        yield \"Please provide values for all fields in all samples.\"\n",
        "        return  # terminate function here to prevent further execution\n",
        "\n",
        "    # Construct examples from the fields and values\n",
        "    sample1 = {\n",
        "        sample_field1: sample1_value1,\n",
        "        sample_field2: sample1_value2,\n",
        "        sample_field3: sample1_value3\n",
        "    }\n",
        "    sample2 = {\n",
        "        sample_field1: sample2_value1,\n",
        "        sample_field2: sample2_value2,\n",
        "        sample_field3: sample2_value3\n",
        "    }\n",
        "    sample3 = {\n",
        "        sample_field1: sample3_value1,\n",
        "        sample_field2: sample3_value2,\n",
        "        sample_field3: sample3_value3\n",
        "    }\n",
        "\n",
        "    # Generate data based on validated inputs\n",
        "    try:\n",
        "        yield from generate_dataset(subject, num_samples, sample1, sample2, sample3)\n",
        "    except Exception as e:\n",
        "        yield f\"Error during data generation: {e}\"\n",
        "        return  # Terminate the function after yielding the error message to prevent further execution"
      ],
      "metadata": {
        "id": "nheHOEtsw1Ki"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate synthetic data\n",
        "def generate_dataset(subject, num_samples, sample1, sample2, sample3):\n",
        "    global tokenizer, model  # Use the globally defined tokenizer and model\n",
        "\n",
        "    # Construct messages for the chat template: the system prompt and user-specific instructions\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": SYSTEM_PROMPT},  # System-level instructions\n",
        "        {\"role\": \"user\", \"content\": user_prompt(subject, num_samples, sample1, sample2, sample3)}  # User-defined parameters\n",
        "    ]\n",
        "\n",
        "    # Prepare the input data for the model using the tokenizer\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",  # Return as PyTorch tensors\n",
        "        add_generation_prompt=True  # Include a generation prompt for the model\n",
        "    ).to(\"cuda\")  # Send data to the GPU for faster processing\n",
        "\n",
        "    # Set up a streamer to handle the token decoding process\n",
        "    streamer = TextIteratorStreamer(\n",
        "        tokenizer,  # The tokenizer used for decoding\n",
        "        skip_prompt=True,  # Skip the prompt text in the output\n",
        "        decode_kwargs={\"skip_special_tokens\": True}  # Exclude special tokens during decoding\n",
        "    )\n",
        "\n",
        "    # Create a thread to handle text generation asynchronously\n",
        "    thread = threading.Thread(\n",
        "        target=model.generate,  # Use the model's generate method\n",
        "        kwargs={\n",
        "            \"inputs\": inputs,  # Provide the prepared input tensors\n",
        "            \"max_new_tokens\": MAX_TOKENS,  # Limit the maximum number of tokens to generate\n",
        "            \"streamer\": streamer  # Stream the generated tokens using the streamer\n",
        "        }\n",
        "    )\n",
        "\n",
        "    thread.start()  # Start the generation thread\n",
        "\n",
        "    full_response = \"\"  # Initialize an empty string to store the full response\n",
        "    for chunk in streamer:  # Iterate over the streamed chunks of generated text\n",
        "        cleaned_chunk = chunk.replace(MODEL_EOS, \"\")  # Remove any end-of-sequence markers\n",
        "        full_response += cleaned_chunk  # Append the cleaned chunk to the full response\n",
        "        yield full_response  # Yield the accumulated response in real-time\n",
        ""
      ],
      "metadata": {
        "id": "nOCYpSE2YTlD"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI function\n",
        "def gradio_ui():\n",
        "    with gr.Blocks() as ui:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # Data Architect\n",
        "\n",
        "        Welcome to the `Data Architect`! Use this tool to create realistic and diverse datasets for testing purposes. Simply provide the required information below, including sample field names and values, and let the system generate synthetic data for you.\n",
        "        <br><br>\n",
        "        \"\"\")\n",
        "\n",
        "        # User inputs and output in a single column\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Define General Information\n",
        "        Provide the subject (e.g., Job Postings, Customer Reviews, Movie Listings) and the number of samples you wish to generate. This will guide the synthetic data generation process.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            subject = gr.Textbox(\n",
        "                label=\"Subject\",\n",
        "                placeholder=\"Enter the subject of the data\",\n",
        "                value=\"\"\n",
        "            )\n",
        "            num_samples = gr.Slider(\n",
        "              minimum=1,\n",
        "              maximum=100,\n",
        "              step=1,\n",
        "              value=6,\n",
        "              label=\"Number of Samples\"\n",
        "          )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Define Sample Fields\n",
        "        Specify the names of the fields you want in your dataset. For example: Name, Age, or Address.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            sample_field1 = gr.Textbox(label=\"Field 1\", placeholder=\"Enter name of Field 1\", value=\"\")\n",
        "            sample_field2 = gr.Textbox(label=\"Field 2\", placeholder=\"Enter name of Field 2\", value=\"\")\n",
        "            sample_field3 = gr.Textbox(label=\"Field 3\", placeholder=\"Enter name of Field 3\", value=\"\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Sample 1\n",
        "        Provide example values for each field in your first sample.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            sample1_value1 = gr.Textbox(label=\"Field 1 - Value\", placeholder=\"Enter value for Field 1\", value=\"\")\n",
        "            sample1_value2 = gr.Textbox(label=\"Field 2 - Value\", placeholder=\"Enter value for Field 2\", value=\"\")\n",
        "            sample1_value3 = gr.Textbox(label=\"Field 3 - Value\", placeholder=\"Enter value for Field 3\", value=\"\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Sample 2\n",
        "        Provide example values for each field in your second sample.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            sample2_value1 = gr.Textbox(label=\"Field 1 - Value\", placeholder=\"Enter value for Field 1\", value=\"\")\n",
        "            sample2_value2 = gr.Textbox(label=\"Field 2 - Value\", placeholder=\"Enter value for Field 2\", value=\"\")\n",
        "            sample2_value3 = gr.Textbox(label=\"Field 3 - Value\", placeholder=\"Enter value for Field 3\", value=\"\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Sample 3\n",
        "        Provide example values for each field in your third sample.\n",
        "        \"\"\")\n",
        "        with gr.Row():\n",
        "            sample3_value1 = gr.Textbox(label=\"Field 1 - Value\", placeholder=\"Enter value for Field 1\", value=\"\")\n",
        "            sample3_value2 = gr.Textbox(label=\"Field 2 - Value\", placeholder=\"Enter value for Field 2\", value=\"\")\n",
        "            sample3_value3 = gr.Textbox(label=\"Field 3 - Value\", placeholder=\"Enter value for Field 3\", value=\"\")\n",
        "\n",
        "        # Button to trigger generation\n",
        "        generate_button = gr.Button(\"Generate Synthetic Data\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ## Generated Synthetic Data\n",
        "        The generated data will appear here in JSON format. You can copy and use it directly for your testing needs.\n",
        "        \"\"\")\n",
        "        output = gr.Markdown(min_height=100)\n",
        "\n",
        "        # Bind button click to processing function\n",
        "        generate_button.click(\n",
        "            process_user_inputs,\n",
        "            [\n",
        "                subject, num_samples,\n",
        "                sample_field1, sample_field2, sample_field3,\n",
        "                sample1_value1, sample1_value2, sample1_value3,\n",
        "                sample2_value1, sample2_value2, sample2_value3,\n",
        "                sample3_value1, sample3_value2, sample3_value3\n",
        "            ],\n",
        "            output\n",
        "        )\n",
        "\n",
        "    return ui\n"
      ],
      "metadata": {
        "id": "pgTXRK7hwUKT"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the UI\n",
        "ui = gradio_ui()\n",
        "ui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "eVs6Bhp_wUGC",
        "outputId": "d2210d41-5c9c-4acb-b33c-3016d692f485",
        "collapsed": true
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://64bb9747b4fe89de09.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://64bb9747b4fe89de09.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contributing\n",
        "Contributions are welcome! Here are some ways you can contribute to the project:\n",
        "- Report bugs and issues.\n",
        "- Suggest new features or improvements.\n",
        "- Submit pull requests with bug fixes or enhancements.\n",
        "\n",
        "You can contribute to this project by visiting the [GitHub repository](https://github.com/emads22/MeetingRecap).\n",
        "\n",
        "## Author\n",
        "- **Emad**  \n",
        "  [<img src=\"https://img.shields.io/badge/GitHub-Profile-blue?logo=github\" width=\"150\">](https://github.com/emads22)\n",
        "\n",
        "## License\n",
        "This project is licensed under the MIT License, which grants permission for free use, modification, distribution, and sublicense of the code, provided that the copyright notice (attributed to [emads22](https://github.com/emads22)) and permission notice are included in all copies or substantial portions of the software. This license is permissive and allows users to utilize the code for both commercial and non-commercial purposes.\n",
        "\n",
        "Please see the [LICENSE](https://github.com/emads22/MeetingRecap/blob/main/LICENSE) file for more details.\n"
      ],
      "metadata": {
        "id": "i7VQkGygFBYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAWsS6HuFKmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}